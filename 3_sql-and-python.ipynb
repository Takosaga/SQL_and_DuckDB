{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76384b6",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/motherduckdb/sql-tutorial\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# 3. Combining SQL and Python\n",
    "\n",
    "## Configuring your execution environment\n",
    "\n",
    "We have conveniently included a devcontainer that will run inside a GitHub codespace on GitHub. This is the approach we will be showing for the next part of this demo. To open the dev container, go to [the repo](https://github.com/motherduckdb/sql-tutorial), click the green button that says `<> Code`, select \"Codespaces\", and create a new one! For the best experience, click the \"+\" and select 4-cores (or more).\n",
    "\n",
    "Once you are in the code space, we want to use a [marimo](https://marimo.io/) notebook. Marimo is similar to Jupyter notebooks, but built in a way that behaves a little bit better for our demo. To get started with marimo, run the following commands in the terminal of your codespace:\n",
    "1. `uv init`\n",
    "2. `uv add marimo`\n",
    "3. `uv run marimo edit`\n",
    "\n",
    "This will spin up a marimo server in our codespace, and open it in a new tab. Make sure to grab your access token from the terminal and you can connect! Create a new notebook, and then lets get back to the tutorial.\n",
    "\n",
    "## A. Using `duckdb` from Python\n",
    "\n",
    "DuckDB is released with a native Python client. You can install it with a simple `pip install` (or `uv add`), and there are no dependencies required.\n",
    "\n",
    "We will also install a few dataframe libraries, but these are optional unless you would like to do some of your analysis outside of DuckDB!\n",
    "\n",
    "```bash\n",
    "# run this in terminal in vs code [or in the marimo UI in \"packages\" section on the left]\n",
    "uv add duckdb pandas polars pyarrow\n",
    "```\n",
    "\n",
    "```bash\n",
    "# run this in terminal in vs code \n",
    "wget https://raw.githubusercontent.com/motherduckdb/sql-tutorial/main/data/ducks.csv -q --show-progress\n",
    "wget https://raw.githubusercontent.com/motherduckdb/sql-tutorial/main/data/birds.csv -q --show-progress\n",
    "wget https://raw.githubusercontent.com/motherduckdb/sql-tutorial/main/answers/answers_3.zip -q \n",
    "unzip -o answers_3.zip -d answers \n",
    "```\n",
    "\n",
    "DuckDB follows the Python DB API spec, so you can use it the same way you would use another database.\n",
    "fetchall() returns a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb334a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "duckdb.execute(\"SELECT 42 as hello_world\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abce8c7",
   "metadata": {},
   "source": [
    "DuckDB also has a .sql method that has some convenience features beyond .execute. We recommend using .sql!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9030aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"SELECT 42 as hello_world\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3329b4",
   "metadata": {},
   "source": [
    "Of course, Marimo has its own sql interaction, which is awesome as well. Click the \"SQL\" Icon in the notebook and you write SQL as if you are inside a SQL-based IDE.\n",
    "\n",
    "```sql\n",
    "SELECT 42 as hello_world\n",
    "```\n",
    "\n",
    "**Note**: When you run functions that have dependencies that are not yet imported, Marimo will detect it and suggest that you add it to your environment. You may notice that it wants to grab `sqlglot` for the SQL cells - it is safe to do so.\n",
    "\n",
    "## B. Writing Pandas DataFrames with DuckDB\n",
    "DuckDB can also return a DataFrame directly using .df(), instead of a list of tuples!\n",
    "\n",
    "This is much faster for large datasets, and fits nicely into existing dataframe workflows like charting or machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"SELECT 42 as hello_world\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc9f5d",
   "metadata": {},
   "source": [
    "You will notice that if we inspect the SQL cell from Part A - Marimo automagically loads your sql query into a dataframe. Neat!\n",
    "\n",
    "## C. Reading Pandas DataFrames\n",
    "Not only can DuckDB write dataframes, but it can read them as if they were a table!\n",
    "\n",
    "No copying is required - DuckDB will read the existing Pandas object by scanning the C++ objects underneath Pandas' Python objects.\n",
    "\n",
    "For example, to create a Pandas dataframe and access it from DuckDB, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ducks_pandas = pd.read_csv('ducks.csv')\n",
    "\n",
    "duckdb.sql(\"SELECT * FROM ducks_pandas\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4fd01",
   "metadata": {},
   "source": [
    "### When to use pd.read_csv?\n",
    "How would you decide whether to use Pandas or DuckDB to read a CSV file? \n",
    "DuckDB has recently invested heavily in its CSV reader and is now able to handle even messier files than Pandas!\n",
    "So our recommendation would be to use DuckDB's CSV reader first (for robustness to odd CSVs, speed, and memory efficiency) and fall back to Pandas.\n",
    "\n",
    "### Checking back in on Marimo\n",
    "\n",
    "If you haven't yet saved your notebook, perhaps now is a good time to do so. Give it a name on the top of your file, and then click the save icon (it should be highlighted yellow). When we drop back to the github codespace, we can see that the `.py` file has been created for us, and if you inspect the file you will notice - its just a python script! This means we can (1) source control it easily and (2) run it as a python script whenever we want. \n",
    "\n",
    "You will also noticed that it uses `mo.sql` for sql cells - this a function in the marimo library that lets us get that nice sql interaction baked into python.\n",
    "\n",
    "## D. Reading and Writing Polars and Apache Arrow\n",
    "\n",
    "In addition to Pandas, DuckDB is also fully interoperable with Polars and Apache Arrow.\n",
    "\n",
    "Polars is a faster and more modern alternative to Pandas, and has a much smaller API to learn.\n",
    "\n",
    "Apache Arrow is *the* industry standard tabular data transfer format. Polars is actually built on top of Apache Arrow data types. Apache Arrow and DuckDB types are highly compatible. Apache Arrow has also taken inspiration from DuckDB's `VARCHAR` data type with their new `STRING_VIEW` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9381e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as pa_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "ducks_polars = pl.read_csv('ducks.csv')\n",
    "duckdb.sql(\"\"\"SELECT * FROM ducks_polars\"\"\").pl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af70aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ducks_arrow = pa_csv.read_csv('ducks.csv')\n",
    "duckdb.sql(\"\"\"SELECT * FROM ducks_arrow\"\"\").arrow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88972f",
   "metadata": {
    "cell_type": "markdown"
   },
   "source": [
    "**Exercise 3.01**\n",
    "\n",
    "Read in the birds.csv file using Apache Arrow, then use the DuckDB Python library to execute a SQL statement on that Apache Arrow table to find the maximum `wing_length` in the dataset. Output that result as an Apache Arrow table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da625ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to show solution\n",
    "# !cat ./answers/answer_3.01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e323267",
   "metadata": {
    "cell_type": "markdown"
   },
   "source": [
    "**Exercise 3.02**\n",
    "\n",
    "\n",
    "Use the DuckDB Python client to return these results as a Polars dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275c538",
   "metadata": {
    "cell_type": "markdown"
   },
   "source": [
    "```sql\n",
    "SELECT\n",
    "    Species_Common_Name,\n",
    "    AVG(Beak_Width) AS Avg_Beak_Width,\n",
    "    AVG(Beak_Depth) AS Avg_Beak_Depth,\n",
    "    AVG(Beak_Length_Culmen) AS Avg_Beak_Length_Culmen\n",
    "FROM 'birds.csv'\n",
    "GROUP BY Species_Common_Name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dbf7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to show solution\n",
    "# !cat ./answers/answer_3.02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b8f43",
   "metadata": {},
   "source": [
    "## 2. Using the DuckDB Relational API\n",
    "\n",
    "In addition to SQL, you can write DuckDB queries using the built-in relational API. \n",
    "We'll see how to chain functions together to build up a query.\n",
    "DuckDB will then combine that function chain together into a single query, so you still get all of DuckDB's query optimization (speed and scalability!) benefits.\n",
    "\n",
    "The question we will build up towards answering is, \"Who were the most prolific people at finding many new species of non-extinct ducks, and when did they get started finding ducks?\"\n",
    "\n",
    "<!-- TODO: Add in a few incremental steps as examples -->\n",
    "\n",
    "The Relational API has a `read_csv` function that mirrors the Pandas function of the same name.\n",
    "These methods can be called on DuckDB connection objects or the default connection using the duckb object itself.\n",
    "\n",
    "As a note, if we wrap our relational expressions in a set of parentheses, we can organize them nicely across multiple lines. \n",
    "\n",
    "Here, we `filter` using an expression much like we would use in a `where` clause to only non-extinct ducks. \n",
    "The, we use `aggregate` to execute a `group by` operation, grouping by the `author` column. \n",
    "Lastly, we `order` by one of the aggregate expression columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "duck_legends = (duckdb\n",
    "  .read_csv(\"ducks.csv\")\n",
    "  .filter(\"extinct = 0\")\n",
    "  .aggregate(\"author, count(name) as count_name, min(year) as min_year\", \"author\")\n",
    "  .order(\"count_name desc\")\n",
    ")\n",
    "duck_legends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21731c69",
   "metadata": {},
   "source": [
    "You can also see the SQL that is generated using with `sql_query()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duck_legends.sql_query())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191f56f",
   "metadata": {},
   "source": [
    "Much like DuckDB can query DataFrames within SQL as if they were tables, it can also do the same for relation objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select * from duck_legends limit 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d875e",
   "metadata": {},
   "source": [
    "Technically, the `.sql` function returns a relation object as well, so you can also chain relational operations after a SQL one also!\n",
    "\n",
    "As a silly example, we could run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91cdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"select * from duck_legends limit 5\").limit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60565a4d",
   "metadata": {},
   "source": [
    "Now we can mix and match SQL and the relational API!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa7220",
   "metadata": {
    "cell_type": "markdown"
   },
   "source": [
    "**Exercise 3.04**\n",
    "\n",
    "As an exercise, use SQL to initially pull the CSV file, but then chain together the remaining Relational operators from the `duck_legends` example above to return the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e595f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to show solution\n",
    "# !cat ./answers/answer_3.03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ff0b2",
   "metadata": {},
   "source": [
    "## 3. Using `ibis` with a DuckDB backend\n",
    "\n",
    "### A. Introduction to Ibis and DuckDB\n",
    "\n",
    "Another option for using DuckDB is Ibis, a powerful Python library that allows you to interact with databases using a DataFrame-like syntax. We'll also show you how to combine the SQL and Ibis so you can get the best of both worlds.\n",
    "\n",
    "Ibis also works across many different databases, so you can write your code once and run it on a variety of database engines.\n",
    "\n",
    "First, let's make sure you have the necessary packages installed. You can install DuckDB and Ibis using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet 'ibis-framework[duckdb]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cfc3be",
   "metadata": {},
   "source": [
    "We are using Ibis in interactive mode for demo purposes. This converts Ibis expressions from lazily evaluated to eagerly evaluated, so it is easier to see what is happening at each step. It also converts Ibis results into Pandas dataframes for nice formatting in Jupyter.\n",
    "\n",
    "For performance and memory reasons, we recommend not using interactive mode in production!\n",
    "\n",
    "We can connect to a file-based DuckDB database by specifying a file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5635058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibis\n",
    "from ibis import _\n",
    "ibis.options.interactive = True\n",
    "\n",
    "con = ibis.duckdb.connect(database='whats_quackalackin.duckdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11f095",
   "metadata": {},
   "source": [
    "We can read in a CSV using Ibis, and it will use the DuckDB `read_csv_auto` function under the hood. This way we get both DuckDB's performance, and clean Python syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69eef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ducks_ibis = ibis.read_csv('ducks.csv')\n",
    "ducks_ibis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde63ed0",
   "metadata": {},
   "source": [
    "The result of the prior read_csv operation is an Ibis object. It is similar to the result of a SQL query - it is not saved into the database automatically.\n",
    "\n",
    "To save the result of our read_csv into the DuckDB file, we create a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5aa2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_ducks = con.create_table('persistent_ducks', obj=ducks_ibis.to_pyarrow(), overwrite=True)\n",
    "persistent_ducks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538fdee",
   "metadata": {},
   "source": [
    "Now that we have a table set up, let's see how we can query this data using Ibis. Let's see how similar it feels...\n",
    "\n",
    "We will answer the same question as with the relational API: \"Who were the most prolific people at finding many new species of non-extinct ducks, and when did they get started finding ducks?\"\n",
    "\n",
    "Use the `filter` function instead of a `where` clause to choose the rows you are interested in. Ibis also uses Python's `==` comparators instead of SQL's single `=`. \n",
    "\n",
    "Pick your columns using the conveniently named `select` function!\n",
    "\n",
    "The `group_by` functions matches well with the `group by` clause.\n",
    "\n",
    "However, Ibis splits the `select` clause into the `select` function and the `aggregate` function when working with a group by. This aligns with the SQL best practice to organize your `select` clause with non-aggregate expressions first, then aggregate expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ec0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "duck_legends = (persistent_ducks\n",
    "  .filter(persistent_ducks.extinct == 0)\n",
    "  .select(\"name\", \"author\", \"year\")\n",
    "  .group_by(\"author\")\n",
    "  .aggregate([persistent_ducks.name.count(), persistent_ducks.year.min()])\n",
    "  .order_by([ibis.desc(\"Count(name)\")])\n",
    ")\n",
    "duck_legends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibis.to_sql(duck_legends)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c808c",
   "metadata": {},
   "source": [
    "### B. Mixing and matching SQL and Ibis\n",
    "\n",
    "If you have existing SQL queries, or want to use dialect-specific features of a specific SQL database, Ibis allows you to use SQL directly!\n",
    "\n",
    "If you want to begin your Ibis query with SQL, you can use `Table.sql` directly.\n",
    "\n",
    "However, we can no longer refer directly to the `persistent_ducks` object later in the expression. We instead need to use the `_` (which we imported earlier with `from ibis import _`), which is a way to build expressions using Ibis's deferred expression API. So instead of `persistent_ducks.column.function()`, we can say `_.column.function()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duck_legends = (persistent_ducks\n",
    "  .sql(\"\"\"SELECT name, author, year FROM persistent_ducks WHERE extinct = 0\"\"\")\n",
    "  .group_by(\"author\")\n",
    "  .aggregate([_.name.count(), _.year.min()]) # Use _ instead of persistent_ducks\n",
    "  .order_by([ibis.desc(\"Count(name)\")])\n",
    ")\n",
    "duck_legends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1d6de",
   "metadata": {},
   "source": [
    "If you want to begin with Ibis, but transition to SQL, first give the Ibis expression a name using the `alias` function. Then you can refer to that as a table in your `Table.sql` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f90dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "duck_legends = (persistent_ducks\n",
    "  .filter(persistent_ducks.extinct == 0)\n",
    "  .select(\"name\", \"author\", \"year\")\n",
    "  .group_by(\"author\")\n",
    "  .aggregate([persistent_ducks.name.count(), persistent_ducks.year.min()])\n",
    "  .alias('ibis_duck') # Rename the result of all Ibis expressions up to this point\n",
    "  .sql(\"\"\"SELECT * from ibis_duck ORDER BY \"Count(name)\" desc\"\"\")\n",
    ")\n",
    "duck_legends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce6afc",
   "metadata": {},
   "source": [
    "And there you go! You've learned:\n",
    "* How to read and write Pandas, Polars, and Apache Arrow with DuckDB\n",
    "* How to use DuckDB's Relational API and how to combine it with SQL\n",
    "* How to use Ibis to run dataframe queries on top of DuckDB\n",
    "* How to see the SQL that Ibis is running on your behalf\n",
    "* How to mix and match SQL and Ibis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b3866",
   "metadata": {
    "cell_type": "markdown"
   },
   "source": [
    "**Exercise 3.04**\n",
    "\n",
    "\n",
    "Convert the SQL query below into an Ibis expression. You are welcome to ignore the column renaming - think of it as a \"stretch-goal\" if you have time! We did not cover how to do that yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8caa833",
   "metadata": {
    "cell_type": "markdown"
   },
   "source": [
    "```sql\n",
    "SELECT\n",
    "    Species_Common_Name,\n",
    "    AVG(Beak_Width) AS Avg_Beak_Width,\n",
    "    AVG(Beak_Depth) AS Avg_Beak_Depth,\n",
    "    AVG(Beak_Length_Culmen) AS Avg_Beak_Length_Culmen\n",
    "FROM 'birds.csv'\n",
    "GROUP BY Species_Common_Name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb86896b",
   "metadata": {
    "cell_type": "markdown"
   },
   "source": [
    "**Exercise 3.04**\n",
    "\n",
    "\n",
    "Hint: Read directly from a csv file - no need to create a persistent table!\n",
    "\n",
    "Hint 2: Ibis uses `mean` instead of `avg`!\n",
    "\n",
    "Hint 3: Ibis aggregate documentation: https://ibis-project.org/reference/expression-tables#ibis.expr.types.relations.Table.aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab17fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to show solution\n",
    "# !cat ./answers/answer_3.04.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
